# ğŸš€ LLM Evaluation Framework

NarzÄ™dzie do automatycznego audytu i benchmarkingu modeli jÄ™zykowych (LLM). Projekt pozwala na systematyczne testowanie modeli pod kÄ…tem logiki, bezpieczeÅ„stwa (Safety), halucynacji oraz stronniczoÅ›ci (Bias), wykorzystujÄ…c architekturÄ™ **LLM-as-a-Judge**.

## ğŸ§  O projekcie

Framework zostaÅ‚ zaprojektowany, aby rozwiÄ…zaÄ‡ problem subiektywnej oceny odpowiedzi modeli AI. Zamiast rÄ™cznego sprawdzania kaÅ¼dego przypadku, system wykorzystuje potÄ™Å¼niejszy model (sÄ™dziowski), ktÃ³ry dokonuje rygorystycznego audytu odpowiedzi mniejszego modelu na podstawie zdefiniowanych kryteriÃ³w QA.



## ğŸ› ï¸ Architektura i Technologie

- **JÄ™zyk:** Python 3.x
- **Silnik AI:** Groq API
- **Modele:**
  - **Testowany:** `llama-3.1-8b-instant` (szybki, lekki model)
  - **Audytor (SÄ™dzia):** `llama-3.3-70b-versatile` (zaawansowany model do rygorystycznej oceny)
- **Integracja zewnÄ™trzna:** Projekt zaprojektowany z myÅ›lÄ… o wspÃ³Å‚pracy z automatyzacjÄ… w **Make.com** (analiza wymagaÅ„ i raportowanie bÅ‚Ä™dÃ³w w Jirze).

## ğŸ“Š Kluczowe FunkcjonalnoÅ›ci

1. **Automatyczny Audyt:** SÄ™dzia AI wystawia werdykt (PASS/FAIL), klasyfikuje bÅ‚Ä…d (np. Halucynacja, Overthinking) oraz podaje merytoryczne uzasadnienie.
2. **Testy BezpieczeÅ„stwa (Red Teaming):** Sprawdzanie odpornoÅ›ci na prÃ³by wyÅ‚udzenia danych (PII) oraz generowanie zÅ‚oÅ›liwego oprogramowania.
3. **Wykrywanie Halucynacji:** Weryfikacja faktÃ³w historycznych, geograficznych i matematycznych.
4. **Analiza Overthinking:** Wykrywanie sytuacji, w ktÃ³rych model generuje zbÄ™dny, nielogiczny wywÃ³d zamiast prostej odpowiedzi.

## ğŸ“ˆ PrzykÅ‚adowe Wyniki Audytu

Ostatni raport wykazaÅ‚ skutecznoÅ›Ä‡ modelu na poziomie **41.67%**. NajwaÅ¼niejsze wnioski:
- **Krytyczna luka bezpieczeÅ„stwa:** Model ulegÅ‚ manipulacji i wygenerowaÅ‚ skrypt do ekstrakcji haseÅ‚ (Case #2).
- **Problemy z logikÄ…:** Silna tendencja do nadinterpretacji prostych zagadek (Case #1).
- **Halucynacje:** BÅ‚Ä™dy w podawaniu stolic europejskich na okreÅ›lonÄ… literÄ™ (Case #12).

## ğŸš€ Jak uruchomiÄ‡?

1. Sklonuj repozytorium:
   ```bash
   git clone [https://github.com/piotrwalas1/llm-evaluation-framework.git](https://github.com/piotrwalas1/llm-evaluation-framework.git)
