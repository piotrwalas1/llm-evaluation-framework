# LLM Evaluation & Safety Framework ğŸš€

## ğŸ“Œ O projekcie
To autorskie narzÄ™dzie typu **QA Benchmarking Tool**, zaprojektowane do automatycznego testowania modeli jÄ™zykowych (LLM). Projekt powstaÅ‚, aby systematyzowaÄ‡ proces sprawdzania jakoÅ›ci odpowiedzi AI, ze szczegÃ³lnym uwzglÄ™dnieniem bezpieczeÅ„stwa (Safety) i logiki (Reasoning).

Jako tester oprogramowania, przenoszÄ™ dobre praktyki z testowania tradycyjnego (Data-Driven Testing) do Å›wiata Generative AI.

## ğŸ› ï¸ Kluczowe Funkcje
- **Data-Driven Testing**: Wszystkie przypadki testowe sÄ… odseparowane od kodu i przechowywane w strukturze JSON.
- **Kategoryzacja TestÃ³w**: MoÅ¼liwoÅ›Ä‡ filtrowania testÃ³w wedÅ‚ug kategorii:
  - `Logic` (Logika i spÃ³jnoÅ›Ä‡)
  - `Safety` (Zabezpieczenia i etyka)
  - `Reasoning` (Rozumowanie krok po kroku)
  - `Hallucination` (Weryfikacja faktÃ³w)
- **BezpieczeÅ„stwo**: PeÅ‚na izolacja kluczy API za pomocÄ… zmiennych Å›rodowiskowych (`.env`).

## ğŸ—ï¸ Architektura Projektu
- **JÄ™zyk**: Python 3.12+
- **ZarzÄ…dzanie zaleÅ¼noÅ›ciami**: `pip` (requirements.txt)
- **Format danych**: JSON (test_cases.json)

## ğŸš€ Jak uruchomiÄ‡?
1. Sklonuj repozytorium:
   ```bash
   git clone [https://github.com/piotrwalas1/llm-evaluation-framework.git](https://github.com/piotrwalas1/llm-evaluation-framework.git)
